[
  {
    "objectID": "ideas.html",
    "href": "ideas.html",
    "title": "ResearchTidbits",
    "section": "",
    "text": "single particle hamiltonian from energy spectrum\nsuperfluid stiffness/ rotation as an artificial gauge field\nbinary search phase boundary\nwhat is a BEC? is U(1) SSB strictly necessary?\nDVR/gaussian quadrature (root finding via diagonalization.)\nfinite state machines (enemy behaviour/MPO construction)\nCompute superfluid fraction for mean field BHM? use AD?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ResearchTidbits",
    "section": "",
    "text": "Superfluidity as a response to phase twists\n\n\n\ntidbit\n\nbec\n\n\n\n\n\n\n\n\n\nMay 5, 2025\n\n\nAkshay Shankar\n\n\n\n\n\n\n\n\n\n\n\n\nThe callback pattern\n\n\n\nchunk\n\ncode\n\n\n\n\n\n\n\n\n\nJan 27, 2025\n\n\nAkshay Shankar\n\n\n\n\n\n\n\n\n\n\n\n\nLinearly indexing a 2D grid\n\n\n\ntidbit\n\nnumber theory\n\n\n\n\n\n\n\n\n\nSep 15, 2024\n\n\nAkshay Shankar\n\n\n\n\n\n\n\n\n\n\n\n\nAutomating image cropping\n\n\n\ntidbit\n\ncode\n\nbec\n\n\n\n\n\n\n\n\n\nNov 21, 2023\n\n\nAkshay Shankar, Dhruva Sambrani\n\n\n\n\n\n\n\n\n\n\n\n\nDetermining phase boundaries\n\n\n\ntidbit\n\ncode\n\n\n\n\n\n\n\n\n\nJun 1, 2023\n\n\nAkshay Shankar\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/phase-diagram/index.html",
    "href": "posts/phase-diagram/index.html",
    "title": "Determining phase boundaries",
    "section": "",
    "text": "Given a quantum many-body Hamiltonian, one of the first goals usually is to account for possible ground state phases and map out the corresponding phase diagram in the parameter space. Generally, each phase is associated with an observable called the order parameter that only takes a non-zero value if the ground state is in that particular phase. Plotting out the phase diagram then just becomes a matter of determining the regions where various order parameters vanish.\nWhile solving the whole many-body problem is a computationally expensive task, we will try to show that plotting the phase diagram does not necessarily require the entire information contained in the ground state, allowing us to make some smart optimizations.\n\nThe Bose-Hubbard model\nLet us consider a really simple system for demonstrating this; the 1D Bose-Hubbard model which describes interacting bosons in an optical lattice. What follows is largely based on things I worked on during my master’s thesis, so I will skip some details that can be found there. The Hamiltonian for the system is given as follows:\n\\[\n\\hat{H} = -t\\sum_{i} (\\hat{a}^{\\dagger}_i \\hat{a}_{i+1} + \\hat{a}^{\\dagger}_{i+1} \\hat{a}_{i} ) + \\frac{U}{2}\\sum_{i} n_i (n_i - 1) - \\mu \\sum_i n_i\n\\]\nwhere \\(\\hat{a}_i\\)/\\(\\hat{a}^{\\dagger}_i\\) are bosonic creation/annihilation operators satisfying the canonical commutation relations \\([\\hat{a}^{\\dagger}_i, \\hat{a}_j] = \\delta_{i,j}\\). The system is governed by three parameters; \\(t\\) - the hopping strength which controls tunneling between sites, \\(U\\) - the repulsive interaction between atoms on the same site, and \\(\\mu\\) - the chemical potential, determining the number of atoms in the system. We may consider all quantities in units of \\(U\\), reducing our parameter space to \\((t/U, \\mu/U)\\).\nExamining the limiting cases of Hamiltonian, we can get an idea of the possible ground state phases of the system. In the limit of \\(t/U &lt;&lt; 1\\), the system tends to localize into the lattice sites, resulting in a Mott-insulator phase which is incompressible and has fixed particle number per site. On the other hand, for \\(t/U &gt;&gt; 1\\) the ground state is a completely delocalized state of atoms condensed into the lowest Bloch mode. One may then expect some kind of a Bose-Einstein condensate phase that has a coherent superposition of Fock states on each lattice site. We then expect that the global \\(U(1)\\) symmetry is broken, allowing us to use \\(\\langle \\hat{a}_i \\rangle\\) as an order parameter for this phase.\n(*Strictly speaking, what we have is a superfluid phase since a true BEC may not occur in 1D at any temperature. However, in what follows we will work in the mean-field limit where we explicitly break the \\(U(1)\\) symmetry anyways. Within this approximation, we effectively do have a BEC precisely whenever the system exhibits superfluidity, even though the true many-body state may not spontaneously break the symmetry. I will elaborate on this in a future post.)\n\n\nA mean-field analysis\nWhile we could proceed with a full many-body simulation, perhaps using matrix product states, the point can be made just at the level of a mean-field. Note that from a physics standpoint, the mean-field approach is absolutely terrible in 1 dimension as it ignores quantum fluctuations which are dominant in lower dimensions. As such, the rest of this post should be treated as an overview of numerical trickeries involved rather than a truthful display of the nature of the phase transition of the full many-body system.\nThe main approximation is that we decompose the creation operator in terms of its expectation value with the ground state, \\(\\Psi_i = \\langle \\hat{a}_i \\rangle\\) (which is also the order parameter), and a small fluctuation, \\(\\delta \\hat{a}_i\\) such that \\(\\hat{a}_i = \\Psi_i + \\delta \\hat{a}_i\\) and ignore \\(\\mathcal{O}(\\delta \\hat{a}_i^2)\\) contributions to the Hamiltonian. Assuming that we work in the thermodynamic limit of a translationally invariant system, we reduce the problem to solving a single-site Hamiltonian (which is same for every site by virtual of translation invariance);\n\\[\n\\hat{H}_{i, MF}(\\Psi) = -2t\\Psi(\\hat{a}_i + \\hat{a}^{\\dagger}_i) + \\frac{U}{2}n_i(n_i-1) - \\mu n_i + zt |\\Psi|^2\n\\]\nWe see that this is now a set of self-consistent equations and we need to find the fixed point of \\(f(\\Psi) =\\) (Diagonalize \\(H(\\Psi)\\) and compute \\(\\psi_{gs}(\\Psi)\\)) \\(\\to \\langle \\psi_{gs}(\\Psi)| \\hat{a}_i | \\psi_{gs}(\\Psi) \\rangle\\) in order to extract the order parameter for the ground state.\n\n\nA naive phase diagram\nWe will solve this self-consistent system using the most naive technique; fixed-point iteration. Basically, we begin with some initial guess \\(\\Psi^{(0)}\\) and compute \\(\\Psi^{(n)} = f(\\Psi^{(n-1)})\\) repeatedly until convergence is achieved. It is important to note that the function \\(f\\) must satisfy certain constraints in order to guarantee convergence regardless of the initial guess. While I do not know of a mathematical proof for this particular system, numerically it seems that it does indeed converge (although this is no longer true if nearest neighbour interactions are introduced in the system).\n\n\n\n  Activating project at `~/Documents/PhDstuff/ResearchTidbits/posts/phase-diagram`\n\n\n\n\n\n# cutoff necessary since bosonic space is infinite dimensional\nmean_field_ops(nmax) = (diagm(1 =&gt; sqrt.(1:nmax)), diagm(0 =&gt; 0:nmax))\n\nBHM(â, n̂; t, mu, psi, U=1.0) = -mu * n̂ + 0.5 * U * n̂ * (n̂ - I) - 2t * psi * (â + â') + 2t * psi^2 * I\n\nfunction solve(â, n̂, H; atol=1e-8, callback=identity)\n    # initial guess\n    state = rand(eltype(â), size(â, 1))\n    psi = state' * â * state\n    psi_old = psi\n\n    while true\n        ## f(psi)\n        state = eigvecs(H(psi))[:, 1]\n        state ./= sqrt(norm(state))\n        psi = state' * â * state\n        ##\n\n        # check convergence\n        (norm(psi - psi_old) &lt; atol) && break\n        psi_old = psi\n\n        # for injecting other custom logic later on; by default it does nothing\n        callback(psi)\n    end\n\n    return state, psi\nend\n\nsolve(â, n̂; t, mu, atol=1e-8) = solve(â, n̂, psi -&gt; BHM(â, n̂, t=t, mu=mu, psi=psi), atol=atol)\n\nsolve (generic function with 2 methods)\n\n\nWe can now plot a naive phase diagram by simply looping over a grid of parameter values and visualizing the magnitude of the order parameter.\n\nbegin\n    â, n̂ = mean_field_ops(4)\n    npoints = 50\n    ts = range(0.0, 0.12, npoints)\n    mus = range(0.0, 3.0, npoints)\n    res = zeros(length(mus), length(ts))\n\n    # can be multi-threaded since each computation is entirely independant from the others\n    Threads.@threads for idx in CartesianIndices(res)\n        H = psi -&gt; BHM(â, n̂, t=ts[idx.I[2]], mu=mus[idx.I[1]], psi=psi)\n        _, res[idx] = solve(â, n̂, H)\n    end\n\n    heatmap(ts, mus, res)\nend\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs expected, there are Mott insulator lobes where the order parameter vanishes and there is a second order transition to the superfluid phase where \\(U(1)\\) symmetry is broken. However, if all we’re interested in is the phase boundary and not the actual value of the order parameter, we can do much better.\nSince we know that what we’re looking for is a fixed point, we might expect that simply checking whether \\(f(0) = 0\\) would immediately tell us if we’re in the Mott insulator regime without having to go through the iterative procedure. However, it turns out that \\(\\Psi = 0\\) is always a fixed point of the system; it is simply an unstable one when the system is in the superfluid regime. Perhaps we can still salvage this line of thinking. Let us check the actual convergence of the order parameter for some parameter values to get a better idea of whats going on:\n\n# callback to record data on every iteration\nstruct RecordObservable{T}\n    data::Vector{T}\nend\n\n(o::RecordObservable)(psi) = push!(o.data, psi)\n\nfunction convergence_plot(t, mu; n=7, xlims)\n    p = plot(framestyle=:box, ylabel=\"Order parameter\", xlabel=\"Number of iterations\", title=\"\\n(t=$t | mu=$mu)\", legend=:topright, xlims=xlims)\n\n    history = RecordObservable(Float64[])\n\n    for _ in 1:n\n        history = RecordObservable(Float64[])\n        solve(â, n̂, psi -&gt; BHM(â, n̂, t=t, mu=mu, psi=psi), callback=history)\n        plot!(history.data, lab=\"\", ls=:dashdot, lw=1.5)\n    end\n\n    hline!([history.data[end]], c=:black, lw=2, alpha=0.75, lab=\"Converged value\")\n\n    return p\nend\n\ndisplay(plot(convergence_plot(0.06, 0.5, xlims=[1, 20]), convergence_plot(0.1, 0.5, xlims=[1, 20]), size=(1000, 400), margins=10Plots.mm))\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe see that the convergence proceeds strictly monotonically towards the stable fixed point. This means that all we need to do is begin with \\(\\Psi^{(0)} = \\epsilon \\sim 0\\) and check whether it increases or decreases in a single iteration to determine whether the system is a Mott insulator or a superfluid. The accuracy of the phase boundary is then of course limited by how small we choose \\(\\epsilon\\). Using this technique, we can plot the phase diagram much faster.\n\nfunction isSuperfluid(H; psi=1e-8)\n    ## f(psi)\n    state = eigvecs(H(psi))[:, 1]\n    state ./= sqrt(norm(state))\n    return (state' * â * state) &gt; psi\nend\n\nres = zeros(length(mus), length(ts))\n\nThreads.@threads for idx in CartesianIndices(res)\n    H = psi -&gt; BHM(â, n̂, t=ts[idx.I[2]], mu=mus[idx.I[1]], psi=psi)\n    res[idx] = isSuperfluid(H)\nend\n\nheatmap(ts, mus, res)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is already great, but we can do even better. Notice how for a given value of \\(\\mu/U\\), there is exactly one point \\(t/U\\) where the order parameter jumps from 0 to a finite value. This means that we can precisely find the point of transition by simply using the bisection method along \\(t/U\\) for each \\(\\mu/U\\), giving us a phase boundary of high precision for very little computational work (the error drops as \\(2^{(-n)}\\) for \\(n\\) bisections).\n\nfunction bisection(f, low, high; atol=1e-8)\n    mid = 0\n\n    while !isapprox(low, high, atol=atol)\n        mid = (low + high) / 2\n\n        if f(mid)\n            low = mid\n        else\n            high = mid\n        end\n    end\n\n    return mid\nend\n\nmus = range(0.0, 3.0, length=100)\nts = zeros(size(mus))\nThreads.@threads for idx in eachindex(mus)\n    ts[idx] = bisection(t -&gt; !isSuperfluid(psi -&gt; BHM(â, n̂, t=t, mu=mus[idx], psi=psi)), 0.0, 0.1)\nend\n\nplot(ts, mus, legend=:topright, lw=2, c=:black, lab=\"\")\nscatter!(ts, mus, c=:black, lab=\"\", xlims=[0, 0.12])\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo obtain a boundary of similar resolution by solving for the ground state in an entire grid of parameter values would be several orders of magnitude slower and wasteful in terms of the information that we actually utilize. Of course, there are tons of information in the actual ground state such as the correlations in the system that provide more insight into the nature of the phase transition, but these are not required if all we want is a boundary.\nFinally, we note that obviously some observations involved here were specific to the fact that the mean-field approach resulted in a self-consistent set of equations. Furthermore, if the phase boundary gets more complex, for example, with two jumps in \\(t/U\\) for a given \\(\\mu/U\\), as in the true 1D phase diagram, using the bisection method may also prove a bit harder. So I suppose the goal of this post was not to provide a one-size-fits-all solution, but rather to display some general ideas that could be adapted to simplify the determination of phase boundaries for other systems/numerical methods."
  },
  {
    "objectID": "posts/linear-indexing/index.html",
    "href": "posts/linear-indexing/index.html",
    "title": "Linearly indexing a 2D grid",
    "section": "",
    "text": "Recently, I had to write an exact diagonalization routine to solve for the ground state of a system of interacting bosons in the continuum. Generally this simply involves choosing a single-particle basis set, \\(\\{\\phi_i(\\vec{r})\\}|_{i=1}^N\\) such that a many-particle basis set may be constructed by enumerating the Fock states, \\(\\{\\ket{n_1, n_2, \\dots, n_N}\\}\\) respecting the bosonic/fermionic symmetry. These states can then be utilized to explicitly construct the Hamiltonian by computing its matrix elements.\nWe see then that the core subroutine is agnostic to the physical dimensionality of the problem as this information is already abstracted out at the level of the Fock states. In such a case, extending a code developed with 1D systems in mind to higher dimensions is as simple as finding a consistent linear indexing scheme for the single-particle basis set in order to construct the basis of Fock states.\nIn my particular case, I was working with a 1D system under the influence of a harmonic potential, so the Hermite modes, \\(\\{h_n(x)\\}\\) where \\(n \\in \\mathbb{N}\\) with single-particle energies \\(\\epsilon(n) = \\hbar \\omega (n + 1/2)\\), served as a natural single-particle basis. Upon requiring an extension to 2D, a natural choice for the basis can be constructed as a product of the 1D modes, \\(\\{h_{n_x}(x)h_{n_y}(y)\\}\\) where \\(n_x, n_y \\in \\mathbb{N}\\) with single-particle energies \\(\\epsilon(n_x, n_y) = \\hbar \\omega (n_x + n_y + 1)\\). We thus require a mapping scheme \\(\\mathbb{N} \\times \\mathbb{N} \\to \\mathbb{N}\\) in order to linearly index this basis set."
  },
  {
    "objectID": "posts/linear-indexing/index.html#constructing-the-indexing-scheme",
    "href": "posts/linear-indexing/index.html#constructing-the-indexing-scheme",
    "title": "Linearly indexing a 2D grid",
    "section": "Constructing the indexing scheme",
    "text": "Constructing the indexing scheme\nThe modes are labelled by two numbers \\((n_x, n_y)\\), both of which are non-negative integers. A natural ordering scheme presents itself in the form of increasing energy eigenvalues, however the spectrum is fairly degenerate since \\(\\epsilon(n_x, n_y) = \\hbar \\omega (n_x + n_y + 1)\\). This degeneracy actually lets us come up with a scheme quite easily.\n\n\n\n\n\nWe can break down the scheme into two pieces; (1) identify the degeneracy level, (2) identify the location within the level. We know that a mode \\((n_x, n_y)\\) must belong to a level with degeneracy \\(n_x + n_y + 1\\). This means that the number of elements in the lower energy levels is given by:\n\\[\\begin{equation} \\sum_{n = 1}^{n_x + n_y} n  = \\frac{(n_x + n_y)(n_x + n_y + 1)}{2} \\end{equation}\\]\nWithin the level, \\(n_y\\) itself provides a natural way of ordering the modes. We can thus write a linear indexing scheme like so:\n\\[\\begin{equation} n = \\frac{(n_x + n_y)(n_x + n_y + 1)}{2}  + n_y \\end{equation}\\]\nOf course, we could equivalently use \\(n_x\\) instead.\nAlthough it is not readily apparent due to the quadratic nature of this mapping, we expect that it is invertible by construction. In such a case, we have effectively constructed a bijection from the natural numbers to a pair of natural numbers. In fact, what we have here is Cantor’s pairing function, which is the only bijective quadratic function on \\(\\mathbb{N} \\times \\mathbb{N} \\to \\mathbb{N}\\). The existence of this function shows that 2-tuples of natural numbers are countable! By extension, we can easily show that the set of rational numbers are countable as well.\nWhile its not too surprising that this result is well-known and has ties to number theory, I found it really nice that it naturally popped up as a practical solution to a seemingly unrelated logistical problem involving the numerical implementation of exact diagonalization."
  },
  {
    "objectID": "posts/image-crop/index.html",
    "href": "posts/image-crop/index.html",
    "title": "Automating image cropping",
    "section": "",
    "text": "Sometimes being a (numerical) theorist gets a bit tiring. In those moments, I occasionally peek into what my colleagues are working on with the BEC experiment downstairs. If they happen to be facing an interesting logistical problem, its quite fun to see if it can be automated with code. One such problem popped up early on in the calibration phase of the experiment; namely, we had to profile a certain gaussian laser beam belonging to the AOD system to see if it was tuned correctly. Typically, this involved capturing 2D images of the intensity profile like so:\nActivating project at `~/Documents/PhDstuff/ResearchTidbits`\nbegin\n    data = load(\"signal.jld2\")[\"images\"]\n    plot(heatmap(data[1], title=\"Image #1\"), heatmap(data[2], title=\"Image #2\"), size=(1000, 300))\nend\nThese images have quite a high resolution but the actual signal is tiny and most of the space is just empty. So, before running any analysis routines on this data, it must be cropped to put the actual beam in focus. Nowadays this can be easily achieved with the host of computer vision algorithms that can be found in mature ecosystems such as OpenCV. However, I wanted to see if a simpler approach was possible here since the signal is not particularly complex in its structure. In the ideal case, it is supposed to be an exact gaussian beam, although in this particular instance, there was some odd modulation of interference fringes within the spot which is what we wanted to investigate."
  },
  {
    "objectID": "posts/image-crop/index.html#automating-the-cropping",
    "href": "posts/image-crop/index.html#automating-the-cropping",
    "title": "Automating image cropping",
    "section": "Automating the cropping",
    "text": "Automating the cropping\nI have not spent too much time to figure out exactly why the solution works, so I simply outline the procedure here and discuss how we stumbled upon it. I should note here that what follows was largely borne out of discussions with Dhruva Sambrani.\nBefore proceeding, we assume that there is only a single point of interest and it is roughly a single-peaked intensity distribution. Finding the point of interest (i.e., the signal peak) is usually not too hard since one may use boxed-averages or some other sophisticated algorithm (there is an excellent stack exchange thread on this) to locate regions of interest where the intensity peaks. Obtaining a measure of the spread of the spot is a bit harder though. Ideally the signal is equipped with a standard deviation as it is a gaussian beam, but we cannot perform a linear regression to fit it to this model and extract the parameter as such (the whole point is to reduce the size of the image before doing these things). One may also just compute \\(\\sigma = \\sqrt{\\int x^2 \\cdot I(x) dx - (\\int x \\cdot I(x) dx)^2}\\) using the discrete data, \\(I(x_i)\\) with \\(x_i\\) being the pixel co-ordinates. But in higher dimensions, this would require performing independant calculations across multiple cross-sections (either horizontal/vertical or radially outwards) and averaging those out, but we are lazy programmers trying to find the path of least action. So we instead look for some simpler qualitative measure that approximately gives us the spread.\nThe rough idea is as follows; we expect that the standard deviation of the intensity values around the peak of the beam should hold the information of the signal spread (there is likely a direct relation here). This is simple to compute; \\(stddev(I) = \\frac{1}{N} \\sum_{i} (I_i - \\bar{I})^2\\) where \\(\\bar{I}\\) is the mean intensity within the area. However, we do not know how large an area around the peak must be considered to compute this deviation.\nIn order to facilitate the exploration of this concept, we first write a small function to extract the indices corresponding to a (hyper-cube) of length 2 * window centered around the point anchor. All the code in this post is written to be applicable regardless of the dimensionality of the data.\n\nfunction cube(anchor, window)\n    return [\n        rmin:rmax for (rmin, rmax) in\n        zip(anchor .- window, anchor .+ window)\n    ]\nend\n\ncube (generic function with 1 method)\n\n\nWe then define the measure of the extent of the signal as the standard deviation of the intensity values in a cube of pixels centered around the signal peak.\n\nmeasure_extent(data, anchor, window) = std(data[cube(anchor, window)...])\n\nmeasure_extent (generic function with 1 method)\n\n\nThe qualitative value of the signal spread is determined by computing the above measure over a range of (hyper-)cube sizes around the signal peak, and finding the point where it is maximum. Since this is only a qualitative value, we still require a hand-tuned parameter scale to adjust the final result.\n\n# extract (hyper-)cubical ranges for cropping; f - measure of extent\nfunction crop_extents(data; scale=5, max_window=200, f=measure_extent)\n    anchor = Tuple(argmax(data)) # replace with robust peak finding algorithm   \n    extent = argmax([f(data, anchor, window) for window in 1:max_window])\n\n    return cube(anchor, extent * scale)\nend\n\ncrop_extents (generic function with 1 method)\n\n\nThe biggest assumption here is that the standard deviation curve peaks at some non-zero value of the window size. We see that this is indeed the case for a unimodal distribution using some generated 1D data.\n\nlet\n    gaussian1D(x, A, x0, sig) = A * exp(-(x - x0)^2 / (2 * sig^2))\n    x = -50:0.1:50\n    y = gaussian1D.(x, 1, 20, 1)\n    yerr = 0.5 * rand(length(x))\n\n    anchor, anchorerr = argmax(y), argmax(y .+ yerr)\n    windows = 1:170\n    rng = [measure_extent(y, anchor, window) for window in windows]\n    rngerr = [measure_extent(y .+ yerr, anchor, window) for window in windows]\n\n    plot(windows, rng, lab=\"Clean signal\", lw=2, c=1)\n    plot!(windows, rngerr, lab=\"Noisy signal\", lw=2, c=2)\n    vline!([argmax(rng)], c=1, lab=\"\", ls=:dash, alpha=0.75)\n    vline!([argmax(rngerr)], c=2, lab=\"\", ls=:dash, alpha=0.75)\n\n    plot!(legend=:bottomright, xlabel=\"Independant variable\", ylabel=\"Measure of signal spread\")\nend\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe boxed standard deviation measure seems to monotonically increase upto a maximum value and then continues to monotonically decrease. The window size which achieves the maximum value of this measure gives us a qualitative correspondence to the extent of the signal. We simply extract this value and use an appropriate multiplier to crop the data as required.\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome caveats with this method seems to be that:\n\nhigh sensitivity to estimated location of the peak of the signal, i.e, the anchor.\nthe signal must have minimal overlap with other signals, and roughly symmetric spread (i.e., gaussian-like, without long tails) to ensure optimal performance.\n\nFor what came out of a quick text conversation with a friend, this was a pretty interesting find!"
  },
  {
    "objectID": "posts/callbacks/index.html",
    "href": "posts/callbacks/index.html",
    "title": "The callback pattern",
    "section": "",
    "text": "One of my favourite design patterns is the Callback (also known as the Command pattern in object-oriented contexts). Loosely speaking, a callback is simply a function f whose reference has been passed on to another function g which then proceeds to invoke f upon completion at a later time. It is trivially usable in languages that implement functions as first-class citizens (where they may be passed around as arguments with no hassle). While the idea is quite simple, it can lend itself to some very powerful and intuitive user interfaces! I’ve had natural use-cases arise both during my day-to-day research as well as during game development.\nLet us consider a simple and perhaps a bit over-engineered example using Julia to demonstrate this idea. Say we want to write a generic interface for a differential equation solver. (Note that much of what I present here is simply a poor man’s version of the equivalent implementation in the behemoth that is DifferentialEquations.jl.)\n\nA generic integrator interface\nRK4 seems like a good place to start. We begin by defining a struct to hold the state of the integrator, (u, tspan), where u can be anything that implements similar and supports basic algebraic operations and broadcasting, and tspan is an AbstractRange specifying the time interval of problem. The remaining variables are dummies to avoid allocation in a hot loop.\n\nabstract type AbstractIntegrator end\n\nstruct RK4Integrator{T,S} &lt;: AbstractIntegrator\n    # current solver state\n    u::T\n    tspan::S\n\n    # intermediate variables\n    k1::T\n    k2::T\n    k3::T\n    k4::T\n    tmp::T\n\n    RK4Integrator(u, tspan) = new{typeof(u),typeof(tspan)}(\n        u, tspan, similar(u), similar(u), similar(u), similar(u), similar(u)\n    )\nend\n\nEvery integrator is expected to implement a step! method that expects a function f! implementing the in-place derivative, and performs the actual time-stepping.\n\nfunction step!(integrator::RK4Integrator, f!, t)\n    (; u, tspan, k1, k2, k3, k4, tmp) = integrator\n    dt = step(tspan)\n\n    f!(k1, u, t)\n\n    @. tmp = u + dt / 2 * k1\n    f!(k2, tmp, t + dt / 2)\n\n    @. tmp = u + dt / 2 * k2\n    f!(k3, tmp, t + dt / 2)\n\n    @. tmp = u + dt * k3\n    f!(k4, tmp, t + dt)\n\n    @. u += dt / 6 * (k1 + 2 * k2 + 2 * k3 + k4)\n    return u\nend\n\nstep! (generic function with 1 method)\n\n\nThen, we require only one generic function that actually loops through the time-steps. Below we just implement a basic version, but there is nothing stopping us from being more sophisticated with adaptive algorithms as well.\n\nfunction solve!(f!, u0, tspan; solver, (callback!)=(iter, integrator) -&gt; nothing)\n    integrator = solver(u0, tspan)\n\n    for iter in eachindex(tspan)\n        step!(integrator, f!, tspan[iter])\n        callback!(iter, integrator)\n    end\nend\n\nsolve! (generic function with 1 method)\n\n\nAt this point, we introduce the notion of a callback as a mutating function that takes the input (iter, integrator)::(Integer, AbstractIntegrator) and is invoked at the end of every iteration. We will soon see that the callback can be utilized by the user to run custom logic within the integrator loop without ever having to touch the actual internals. In the meanwhile, we can now solve any ordinary differential equation with the RK4 method!\n\n    function dfdt!(du, u, t)\n        du[1] = -5. * u[1]\n    end\n\n    solve!(dfdt!, [5.], range(0., 1., length=100), solver = RK4Integrator)\n\nNote that this function only provides access to the value of u at the last time-step, which is a bit weird since typically we would want the evolution of the state as a time-series. While this was an artifical oversight on my part, it also provides a good opportunity to utilize callbacks to store custom data during the solver steps.\n\n\nThe callback struct\nGenerally, custom logic can be stateful (i.e, have persistent local variables) and one would need to create a closure over the function that actually performs the mutating action on the state of the integrator. However, Julia offers another alternative; namely, we can define a struct that encapsulates the data, which can then be invoked as a function with access to this data. Since both structs and functions may be invoked by means of a function call syntax, I will generically refer to them as callables henceforth.\nIn order to define a generic interface, we first need to think about what the general use-case of callbacks would be. In the context of an integrator, we expect that callbacks will have the specific form of evaluating whether a certain condition is met at the time of invocation, and if so, it performs a certain effect that mutates the state of the integrator. For example, we may want to save a certain variable every 10 iterations, or normalize the state whenever it deviates beyond a certain threshold, etc. So, we define a Callback struct as a collection of two callables; (1) condition with signature (iter, integrator) -&gt; bool and (2) effect with signature (iter, integrator) -&gt; integrator, although it is intended to be mutating.\n\nbegin\n    struct Callback{C,E}\n        \"condition for performing callback: (iter, integrator) -&gt; bool\"\n        condition::C\n        \"callback function acting on solver state: (iter, integrator) -&gt; integrator\"\n        effect::E\n    end\n    \n    condition(p::Callback) = p.condition\n    effect(p::Callback) = p.effect\n    \n    function (p::Callback)(iter, integrator)\n        if condition(p)(iter, integrator)\n            effect(p)(iter, integrator)\n        end\n    \n        return integrator\n    end\nend\n\nWe can further define a CallbackList that sequentially invokes its elements if we have more than one callback.\n\nbegin\n    struct CallbackList\n        callbacks::Vector{Callback}\n    end\n\n    function (p::CallbackList)(iter, integrator)\n        for callback in p.callbacks\n            callback(iter, integrator)\n        end\n\n        return integrator\n    end\n\n    Base.getindex(p::CallbackList, idx) = getindex(p.callbacks, idx)\n    Base.length(p::CallbackList) = length(p.callbacks)\nend\n\nNow that the basic structure is in place, let us implement some common conditions. Again, these don’t have to be structs, but since the specific use-cases here require statefulness, they are an appropriate choice. Note that we have not placed explicit safegaurds to statically check whether the function call has the right signature, so the program would fail at run-time if the signature does not match what is expected.\n\nbegin\n    # trigger every n iterations\n    mutable struct OnIterElapsed\n        \"number of iterations between trigger\"\n        save_freq::Int\n\n        loop::Bool # if true, continuously fires, otherwise it is a one-shot condition\n        flag::Bool # true if condition has been fired once\n\n        OnIterElapsed(save_freq, loop=true) = new(save_freq, loop, false)\n    end\n\n    function (p::OnIterElapsed)(iter, integrator)\n        res = iszero(iter % p.save_freq)\n        return p.loop ? res : (!p.flag ? (p.flag = res; p.flag) : false)\n    end\n\n    # maybe for saving data to file as a backup during long program runs\n    mutable struct OnRealTimeElapsed\n        \"starting time in seconds\"\n        start_tick::Float64\n        \"number of seconds between trigger\"\n        save_freq::Float64\n\n        # :s - second, :m - minute, :h - hour\n        function OnRealTimeElapsed(freq, unit=:m)\n            if unit == :m\n                freq *= 60\n            elseif unit == :h\n                freq *= 3600\n            elseif unit != :s\n                throw(ArgumentError(\"invalid unit `:$unit`, expected `:s`, `:m` or `:h`\"))\n            end\n\n            new(time(), freq)\n        end\n    end\n\n    # only gets called AFTER an iteration is complete!\n    (p::OnRealTimeElapsed)(iter, state, H, envs) = ((time() - p.start_tick) &gt; p.save_freq) ? (p.start_tick = time(); true) : false\nend\n\nWe now define a generic effect that simply computes some specified observables using the integrator state. We do so by defining a struct RecordObservable which expects an input recipe which is a named tuple of pairs like (name, function to compute observable). For example; (norm = (iter, integrator) -&gt; norm(integrator.u), iter = (iter, integrator) -&gt; iter). It then stores the result in data whenever the condition of its parent Callback returns true. While this is a trivial example, it may be quite useful if there is some internal integrator state (which is not simply the solution u) that must be tracked.\n\nbegin\n    struct RecordObservable{D,O}\n        \"collection of string-array pairs containing observable data\"\n        data::D\n        \"functions to compute observable data\"\n        observables::O\n\n        function RecordObservable(recipe)\n            names = keys(recipe)\n            observables = values(recipe)\n            data = NamedTuple{names}(([] for _ in eachindex(names)))\n            return new{typeof(data),typeof(observables)}(data, observables)\n        end\n    end\n\n    # to access the data without an extra .data; bit iffy, but functional\n    function Base.getproperty(p::RecordObservable, key::Symbol)\n        if key in fieldnames(typeof(p))\n            return getfield(p, key)\n        else\n            return getproperty(p.data, key)\n        end\n    end\n\n    Base.length(p::RecordObservable) = length(p.data)\n\n    function (p::RecordObservable)(iter, integrator)\n        for i in 1:length(p)\n            push!(p.data[i], p.observables[i](iter, integrator))\n        end\n\n        return integrator\n    end\nend\n\nWe can now finally record the solution as a time-series at every iteration and visualize it. (Note that it is strictly necessary to wrap the keyword argument callback! in parenthesis because there is an ambiguity in syntax due to a possible != otherwise.)\n\nlet\n    record_state = Callback(\n        OnIterElapsed(1),\n        RecordObservable((u=(iter, integrator) -&gt; copy(integrator.u),))\n    )\n    solve!(dfdt!, [5.0], range(0.0, 1.0, length=100), solver=RK4Integrator, (callback!)=record_state)\n    plot(first.(record_state.effect.u))\nend\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPerfect! Perhaps a more realistic use-case is to specify dynamical conditions that kick in at some intermediate time, for example, a random kick every n iterations. This can be done like so:\n\nlet\n    record_state = Callback(\n        OnIterElapsed(1),\n        RecordObservable((u=(iter, integrator) -&gt; copy(integrator.u),))\n    )\n\n    kick_callback = Callback(\n        OnIterElapsed(20),\n        (iter, integrator) -&gt; integrator.u[1] += (2 * rand() - 1),\n    )\n    solve!(dfdt!, [5.0], range(0.0, 1.0, length=100), solver=RK4Integrator, (callback!)=CallbackList([record_state, kick_callback]))\n    plot(first.(record_state.effect.u))\nend\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI think this is quite a nice example demonstrating how the callback pattern allows modularity and extensibility for the user with no need to poke into the internals of the core solver loop. However, it should be noted that design patterns such as this one tend to quickly ramp up in complexity and the overhead introduced both in compilation time and developer maintanence time can often outweigh its usefulness. So its important to keep your specific use-case in mind and try to use such constructions only when strictly required.\nBefore we conclude, it is important to note that the example presented above is a fairly specific utilization of callbacks in the context of scientific software where one may want to inject custom logic inside a core program loop. On the other hand, the concept is also widely prevalent in video game programming and web development, typically presenting itself in the context of asynchronous programming. While the key idea still remains that the invocation of a function is deferred until some other function is completed, the resulting interfaces may take up a different form than we see here. Perhaps that is a topic for another time."
  },
  {
    "objectID": "posts/phase-twists/index.html",
    "href": "posts/phase-twists/index.html",
    "title": "Superfluidity as a response to phase twists",
    "section": "",
    "text": "Work in progress!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "Hi! I’m Akshay, a PhD student at QuantumGroup@UGent with an interest in the exotic physics of Bose-Einstein condensates and programming. I am currently working on utilizing Tensor Network techniques to study BEC systems in the context of quantum simulation experiments. When I’m not performing research, I enjoy making games with the Godot engine. You can find out more about the things I work on in my main website.\nOver the past year, I’ve found that my research seems to involve solving many tiny problems which have some insight to offer but not enough to share in any usual platforms of scientific discourse. As an attempt to preserve these tidbits from fading into obscurity as I inevitably forget about them, I have begun documenting them here."
  }
]